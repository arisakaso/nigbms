defaults:
  - common
  - data: poisson1d_offline_datamodule
  - meta_solver: poisson1d_meta_solver_default
  - solver: pytorch_jacobi_default
  # - solver: petsc_jacobi_default  
  - surrogate: poisson1d_surrogate_default
  - wrapper: wrapped_solver_default

wandb: 
  mode: disabled # must be onoine when sweep

data:
  data_dir: /workspaces/nigbms/data/raw/poisson1d/small
  dataset_sizes:
    train: 10_000
    val: 10_000
    test: 10_000
  batch_size: 256
  # rtol_dists:
  #   train:
  #     value: 1.0e-6
  # out_task_type: 
  #   _target_: hydra.utils.get_class
  #   path: nigbms.modules.tasks.PETScLinearSystemTask

meta_solver:
  params_learn: 
    x0: 
      - ${latent_dim}
  features:
    b: 
      - ${dim}
  model:
    n_layers: 1
    n_units: 512
    init_weight: 
      dist: uniform
      scale: 1.0e-2
  
solver:
  params_fix: 
    history_length: 501
  params_learn: 
    x0: 
      - ${dim}
      - 1

surrogate:
  params_fix: ${solver.params_fix}
  params_learn: ${solver.params_learn}
  features:
    # b:
    #   - ${dim}
    # x0:
    #   - ${dim}
    # x:
    #   - ${dim}
    # e0:
    #   - ${dim} 
    e0_fft:
      - 32
    # e0_enc:
    #   - ${latent_dim}
  model:
    _target_: nigbms.modules.models.ExponentialDecay
    out_dim: ${solver.params_fix.history_length}
    n_components: 64
    n_units: 1024
    n_layers: 2
    hidden_activation:
      _target_: torch.nn.SiLU
    output_activation: 
      _target_: torch.nn.Identity
    init_weight: null
    init_scale: 1.0e+0
      


constructor:
  _target_: nigbms.modules.constructors.ThetaConstructor
  params: 
    x0:
      codec: 
        _target_: nigbms.modules.constructors.LinearCodec
        param_dim: ${dim}
        latent_dim: ${latent_dim}
      shape: ${solver.params_learn.x0}
    

wrapper:
  hparams:
    opt:
      _target_: torch.optim.Adam
      lr: 1.0e-3
    loss:
      weights:
        y_loss: 0
        y_loss_relative: 0
        dvf_loss: 1
        dvf_loss_relative: 0
        dvL_loss: 0
      mask: True
    clip: 10
    grad_type: cv_fwd
    jvp_type: forwardAD
    eps: 1.0e-10
    Nv: 1
    v_scale: 1.0
    v_dist: rademacher
    
loss:
  _target_: nigbms.modules.losses.MetaSolverLoss
  weights:
    iter_r_proxy: 1
    # e0: 1
    # r0: 1
  reduce: True
  gain: 1.0e+3 # equal to 1/rtol
  
opt:
  _target_: torch.optim.Adam
  lr: 0

sch:
  _target_: torch.optim.lr_scheduler.MultiStepLR
  milestones: [1000000]
  gamma: 0.1
  
callbacks:
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: ${monitor}
    patience: 10
    mode: min
    verbose: True
  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: epoch
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: ${monitor}
    mode: min
    save_top_k: 0
    save_last: ${test}
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: surrogate/sim
    patience: 10000
    verbose: True
    check_finite: True
    check_on_train_epoch_end: True
  

trainer:
  max_epochs: 20
  accelerator: gpu
  devices: 
    - 0
  check_val_every_n_epoch: 20
  fast_dev_run: False
  # limit_train_batches: 2
  # limit_val_batches: 2
  # limit_test_batches: 2

monitor: val/iter_r
test: True
dim: 31
latent_dim: 32
compile: False
logging: True
warmup: 0